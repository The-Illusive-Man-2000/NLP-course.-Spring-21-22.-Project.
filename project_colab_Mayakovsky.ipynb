{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ПРИМЕЧАНИЕ.**\n",
        "\n",
        "Целью проекта не было побить SOTA результаты.\n",
        "Также целью я не ставил сделать этот мир лучше, создав, например, перводчик с русского на суахили.\n",
        "Несколько месяцев назад мне на глаза попалась статья о том, как использовали GPT\n",
        "дообучив модель на тексте переписки умершей девушки. Статья запала в душу.\n",
        "И мне захотелось, что называется \"потрогать\" эту технологию.\n",
        "Но из-за нехватки времени не доходили руки.\n",
        "Потом я записался на этот курс по NLP и решил в рамках курса попробовать самому\n",
        "дообучить модель. Так как использование чужой перписки для публичного проекта\n",
        "неэтично, то мной было принято решение использовать стихи известного поэта.\n",
        "Я начал с Пушкина, но потом решил сменить его на Маяковского, так как у последнего более узнаваемый стиль.\n",
        "К моему сожалению использовать в colab большую и среднюю модель не получилось, \n",
        "пришлось ограничиться маленькой((.\n",
        "В конце этого файла идут ссылки на некоторые статьи, которые мне пришлось читать в рамках этого проекта. \n",
        "\n",
        "\n",
        "P.S. К перписке с другим человеком я еще вернусь, но сделаю это для личного пользования, непублично.\n",
        "\n",
        "Вот ссылка на ту статью.\n",
        "https://habr.com/ru/news/t/576952/"
      ],
      "metadata": {
        "id": "z5YRdBhqvfIc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KhNE10PXuUu",
        "outputId": "e11a6fdb-1d54-4c5a-8138-3af22f9035b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z15lOMU2yk6M"
      },
      "outputs": [],
      "source": [
        "#!git clone  https://github.com/sberbank-ai/ru-gpts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r75h4ESLyk6R",
        "outputId": "d7139ec7-ca95-4ac8-b9f5-6fe4efe79d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5Eezvzm_hrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a7789a-438a-41d7-a104-c4b84a884ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)\n",
        "#torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha9HPro0eQWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c9b6fa-6e59-4d79-ca9f-10a2f1a0cf21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/1b36eeb1fd7b3a6ec11bf46bde2c38e7e68f71ec774694b9e886c86001aab35d.c483bc3440d25937fdac74506b73b76ee6e67f778a804756214363fc2a1a66ef\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/479aa59074c4dcd4c36106252da033d03bc92e3010947ce1d3714de224c2af1f.7362c0dbb32f750eeea5a5b93bbd0c6876eac41453369265d5a49df1c9142b6f\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06f48b6b3173390d047e15d691fda67ae4ea7733a5eea4b6e0115f5099c4e700.b5cdfa39c63384f94159c36bc9042660c747cea5cf520b43d543bd2c68b3164d\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06f48b6b3173390d047e15d691fda67ae4ea7733a5eea4b6e0115f5099c4e700.b5cdfa39c63384f94159c36bc9042660c747cea5cf520b43d543bd2c68b3164d\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/df2b64a4c86a349ba84354d85b7117b106f2b87085c9bb54cde70d3751907c45.4e3da19dd8adaa6d6a9804bfd45d2dcf17ba544de445847443ef1816bfa3d693\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Воспользуемся русскоязычной версией GPT от Сбера.\n",
        "# Large и medium к сожалению, бесплатный colab не тянул,\n",
        "# поэтому придётся пользоваться версией small (((. \n",
        "\n",
        "model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "\n",
        "#model_name_or_path = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
        "#model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soPxrXgQpFZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62dc7e8-7666-45d4-93f9-aad8a6f7e41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "Loading features from cached file drive/MyDrive/Colab Notebooks/ods/cached_lm_GPT2Tokenizer_64_majakovskiy.txt [took 0.001 s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "\n",
        "# Первоначально я дообучал модель на стихах Пушкина.\n",
        "# Но потом решил переключиться на Маяковского, так как у него более узнаваемый стиль.\n",
        "\n",
        "train_path = 'drive/MyDrive/Colab Notebooks/ods/majakovskiy.txt'\n",
        "\n",
        "# Создание датасета\n",
        "train_dataset = TextDataset(tokenizer=tokenizer,file_path=train_path,block_size=64)\n",
        "#block_size: It refers to the windows size that is moved across the text file. Set to -1 to use maximum allowed length.\n",
        "\n",
        "# Создание даталодера (нарезает текст на оптимальные по длине куски)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQH4wBW_pN2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701d3f73-2348-45cc-8d86-4783647e2195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \n",
        "    output_dir=\"drive/MyDrive/Colab Notebooks/ods/output\",\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    \n",
        "    #logging_strategy=\"epoch\",\n",
        "    num_train_epochs=150, # number of training epochs\n",
        "    per_device_train_batch_size=32, # batch size for training\n",
        "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
        "    warmup_steps=10,# number of warmup steps for learning rate scheduler Number of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio.\n",
        "    gradient_accumulation_steps=16, # to make \"virtual\" batch size larger Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5),None) # Optimizer and lr scheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVtvq4sLq9EG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "f1ddf2a4-a6b7-4cf0-f60c-f4e9297fc44f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 24\n",
            "  Num Epochs = 150\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 150\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 01:01, Epoch 150/150]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=150, training_loss=0.09098412831624349, metrics={'train_runtime': 61.8709, 'train_samples_per_second': 58.186, 'train_steps_per_second': 2.424, 'total_flos': 117581414400000.0, 'train_loss': 0.09098412831624349, 'epoch': 150.0})"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "#torch.cuda.empty_cache()\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#print(trainer.state.log_history)\n",
        "#print(type(trainer.state.log_history))\n",
        "#print(len(trainer.state.log_history))\n",
        "#loss_list = []\n",
        "#for element in trainer.state.log_history[:100]:\n",
        "\n",
        "#  loss_list.append(element['loss'])\n",
        "\n",
        "#x = [i for i in range(0,100)]\n",
        "#y = loss_list\n",
        "#plt.plot(x, y)\n",
        "#plt.show() "
      ],
      "metadata": {
        "id": "KJmo_EncvImG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1Vixh9j83ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d79b9a-dbf2-4198-ca2e-a649227f3de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150/config.json\n",
            "Model weights saved in drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "# Сохраняем модель\n",
        "model.save_pretrained(\"drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZfBN1meC79e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0866b04d-0292-4285-c091-7dc271e1c0d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading weights file drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Загружаем сохраненную модель\n",
        "model2 = GPT2LMHeadModel.from_pretrained(\"drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150\").to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Теперь попробуем генерацию текста и посмотрим, получилось ли скопировать стиль Маяковского с короткими, отрывистыми фразами.**\n",
        "Так как метрики bertscore и BLEURT требуют для вычисления два текста, сгенерированный и тот с которым будем сравнивать, а у меня не задача перевода или перефразирования текста, а что назыается генерация с нуля, то придётся оценивать качество сгенерированного моделью текста, что называется \"на глаз\"."
      ],
      "metadata": {
        "id": "rbjCjXHmqS9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=80"
      ],
      "metadata": {
        "id": "j5eYYdPEme-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1)  Greedy Search он же аргмаксная генерация. Выбирается токен, у которого максимальная вероятность.\n",
        "# Не лучший метод, так как часто приводит повторению слов. \n",
        "\n",
        "text = \"В лесу родилась елочка\\n\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "model2.eval()\n",
        "with torch.no_grad():\n",
        "    out = model2.generate(input_ids, do_sample=False, max_length=max_length)\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "w5eJW9-uqF-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8b29d2-f2ba-4ab2-c4ce-dee9455241ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "В лесу родилась елочка\n",
            "и в дорогу\n",
            "убежала елочка.\n",
            "К нам в гости\n",
            "пришел Новый год.\n",
            "Он большой, красивый,\n",
            "и добрый.\n",
            "Снеговик\n",
            "убежал, хлопнув дверью.\n",
            "А у нас\n",
            "в доме\n",
            "взорвался бидон\n",
            "с соленых огурцов.\n",
            "Все в доме\n",
            "плесне\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2)  Beam search он же поиск по лучу. Здесь на каждом шаге выбирается несколько\n",
        "# токенов (зависит от ширины луча) и дальше поиск продолжается для каждого из токенов.\n",
        "# В итоге выбирается вариант с лучшей перплексией.\n",
        "\n",
        "text = \"В лесу родилась елочка\\n\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model2.generate(input_ids, do_sample=True, num_beams=4, max_length=max_length)\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "DQRYPIaYrU5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7cb8d9f-194f-4c38-ae51-c9d066b5d86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "В лесу родилась елочка\n",
            "и в дорогу\n",
            "убежала елочка.\n",
            "К нам в гости\n",
            "пришел Дед Мороз\n",
            "и сказал:\n",
            "— Детишки,\n",
            "каждый день\n",
            "над крышами\n",
            "горизонтов\n",
            "проносятся рой\n",
            "червей и комаров.\n",
            "Каждую ночь\n",
            "над крышами\n",
            "горизонтов\n",
            "проносятся\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Сэмплирование с температурой. Используется для придания тексту \"человечности\"\n",
        "# и непредсказуемости. В этом случае берется не самый вероятный токен, а выбор \n",
        "# делается \"случайно\" с учетом распределения вероятности. Параметр температуры\n",
        "# позволяет контролировать степень случайности. При нулевой температуре получается аргмаксное сэмплирование.\n",
        "# А при большой наугад. Советуют использовать температуру от 0.8 до 2.\n",
        "\n",
        "text = \"В лесу родилась елочка\\n\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model2.generate(input_ids, do_sample=True,temperature=1.1, max_length=max_length)\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "AiFIfsUAsPOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36216dd5-d658-4e4b-9ed8-a98d21353ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "В лесу родилась елочка\n",
            "и во все двери\n",
            "вежливыми\n",
            "нотами\n",
            "сообщают:\n",
            "это —\n",
            "плохо\n",
            "и вполне\n",
            "даже\n",
            "плохо\n",
            "для детей…\n",
            "Все, что нужно\n",
            "домашнему ребенку,\n",
            "достает\n",
            "мама\n",
            "из погреба,\n",
            "а маленький ребенок\n",
            "достает\n",
            "дрожалку\n",
            "и начинает\n",
            "поить\n",
            "мою\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Nucleus Sampling. Сэмплирование с ограничением маловероятных токенов.\n",
        "# Для запрета сэмплирования маловероятных токенов вводят top-k или top-p ограничения.\n",
        "# При top-k зануляются все вероятности кроме k самых больших.\n",
        "# При top-p остается набор токенов, сумма вероятностей которых не больше p. \n",
        "\n",
        "text = \"В лесу родилась елочка\\n\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model2.generate(input_ids, do_sample=True,temperature=1.2, top_k=25,\n",
        "                     top_p=0.8, max_length=max_length)\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "wbNyRgiqtoXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceff14ce-4d30-4c31-8524-e09baef91cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "В лесу родилась елочка\n",
            "и в окошко\n",
            "звездою\n",
            "вылетел зайчик.\n",
            "И все засмеялись\n",
            "над зайчиком\n",
            "и назвали его\n",
            "Зайчиком.\n",
            "\n",
            "Зайчик\n",
            "веселится,\n",
            "улыбаясь,\n",
            "и говорит:\n",
            "«Зайка,\n",
            "я очень маленький,\n",
            "пойду\n",
            "к тебе\n",
            "и сразу спрячусь.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Смысл получившихся текстов конечно оставляет желать лучшего, так как представляет собой шизофрению. Но сети удалось \"ухватить\" стиль Маяковского с \n",
        "короткими, отрывистыми фразами. Возможно с более крупной моделью в тексте было бы больше смысла. "
      ],
      "metadata": {
        "id": "CAL7Kv8zGJE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Статья про BLEURT\n",
        "https://neurohive.io/ru/novosti/bleurt-metrika-dlya-ocenki-modelej-dlya-generacii-teksta/ \n",
        "\n",
        "Метрики, сэплирование.\n",
        "https://spb.hse.ru/mirror/pubs/share/480745430.pdf   \n",
        "\n",
        "Бертскор\n",
        "https://colab.research.google.com/drive/1kpL8Y_AnUUiCxFjhxSrxCsc6-sDMNb_Q  \n",
        "\n",
        "Бертскор\n",
        "https://paperswithcode.com/paper/bertscore-evaluating-text-generation-with \n",
        "\n",
        "Статья про генерацию текста\n",
        "https://huggingface.co/blog/how-to-generate\n",
        "\n",
        "Бертскор\n",
        "https://github.com/huggingface/datasets/blob/master/metrics/bertscore/bertscore.py\n",
        "\n",
        "Fine-tune a non-English GPT-2 Model with Huggingface\n",
        "https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb"
      ],
      "metadata": {
        "id": "YhkTF24by-yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zssXL0SizPgo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "project_colab_Mayakovsky.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}