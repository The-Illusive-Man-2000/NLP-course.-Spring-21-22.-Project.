{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**–ü–†–ò–ú–ï–ß–ê–ù–ò–ï.**\n",
        "\n",
        "–¶–µ–ª—å—é –ø—Ä–æ–µ–∫—Ç–∞ –Ω–µ –±—ã–ª–æ –ø–æ–±–∏—Ç—å SOTA —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.\n",
        "–¢–∞–∫–∂–µ —Ü–µ–ª—å—é —è –Ω–µ —Å—Ç–∞–≤–∏–ª —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ—Ç –º–∏—Ä –ª—É—á—à–µ, —Å–æ–∑–¥–∞–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–≤–æ–¥—á–∏–∫ —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ —Å—É–∞—Ö–∏–ª–∏.\n",
        "–ù–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Å—è—Ü–µ–≤ –Ω–∞–∑–∞–¥ –º–Ω–µ –Ω–∞ –≥–ª–∞–∑–∞ –ø–æ–ø–∞–ª–∞—Å—å —Å—Ç–∞—Ç—å—è –æ —Ç–æ–º, –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ GPT\n",
        "–¥–æ–æ–±—É—á–∏–≤ –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ–∫—Å—Ç–µ –ø–µ—Ä–µ–ø–∏—Å–∫–∏ —É–º–µ—Ä—à–µ–π –¥–µ–≤—É—à–∫–∏. –°—Ç–∞—Ç—å—è –∑–∞–ø–∞–ª–∞ –≤ –¥—É—à—É.\n",
        "–ò –º–Ω–µ –∑–∞—Ö–æ—Ç–µ–ª–æ—Å—å, —á—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"–ø–æ—Ç—Ä–æ–≥–∞—Ç—å\" —ç—Ç—É —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é.\n",
        "–ù–æ –∏–∑-–∑–∞ –Ω–µ—Ö–≤–∞—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–∏ –Ω–µ –¥–æ—Ö–æ–¥–∏–ª–∏ —Ä—É–∫–∏.\n",
        "–ü–æ—Ç–æ–º —è –∑–∞–ø–∏—Å–∞–ª—Å—è –Ω–∞ —ç—Ç–æ—Ç –∫—É—Ä—Å –ø–æ NLP –∏ —Ä–µ—à–∏–ª –≤ —Ä–∞–º–∫–∞—Ö –∫—É—Ä—Å–∞ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–∞–º–æ–º—É\n",
        "–¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å. –¢–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —á—É–∂–æ–π –ø–µ—Ä–ø–∏—Å–∫–∏ –¥–ª—è –ø—É–±–ª–∏—á–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞\n",
        "–Ω–µ—ç—Ç–∏—á–Ω–æ, —Ç–æ –º–Ω–æ–π –±—ã–ª–æ –ø—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∏—Ö–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ –ø–æ—ç—Ç–∞.\n",
        "–Ø –Ω–∞—á–∞–ª —Å –ü—É—à–∫–∏–Ω–∞, –Ω–æ –ø–æ—Ç–æ–º —Ä–µ—à–∏–ª —Å–º–µ–Ω–∏—Ç—å –µ–≥–æ –Ω–∞ –ú–∞—è–∫–æ–≤—Å–∫–æ–≥–æ, —Ç–∞–∫ –∫–∞–∫ —É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –±–æ–ª–µ–µ —É–∑–Ω–∞–≤–∞–µ–º—ã–π —Å—Ç–∏–ª—å.\n",
        "–ö –º–æ–µ–º—É —Å–æ–∂–∞–ª–µ–Ω–∏—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ colab –±–æ–ª—å—à—É—é –∏ —Å—Ä–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, \n",
        "–ø—Ä–∏—à–ª–æ—Å—å –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å—Å—è –º–∞–ª–µ–Ω—å–∫–æ–π((.\n",
        "–í –∫–æ–Ω—Ü–µ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–¥—É—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–Ω–µ –ø—Ä–∏—à–ª–æ—Å—å —á–∏—Ç–∞—Ç—å –≤ —Ä–∞–º–∫–∞—Ö —ç—Ç–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞. \n",
        "\n",
        "\n",
        "P.S. –ö –ø–µ—Ä–ø–∏—Å–∫–µ —Å –¥—Ä—É–≥–∏–º —á–µ–ª–æ–≤–µ–∫–æ–º —è –µ—â–µ –≤–µ—Ä–Ω—É—Å—å, –Ω–æ —Å–¥–µ–ª–∞—é —ç—Ç–æ –¥–ª—è –ª–∏—á–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, –Ω–µ–ø—É–±–ª–∏—á–Ω–æ.\n",
        "\n",
        "–í–æ—Ç —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ç—É —Å—Ç–∞—Ç—å—é.\n",
        "https://habr.com/ru/news/t/576952/"
      ],
      "metadata": {
        "id": "z5YRdBhqvfIc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KhNE10PXuUu",
        "outputId": "e11a6fdb-1d54-4c5a-8138-3af22f9035b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z15lOMU2yk6M"
      },
      "outputs": [],
      "source": [
        "#!git clone  https://github.com/sberbank-ai/ru-gpts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r75h4ESLyk6R",
        "outputId": "d7139ec7-ca95-4ac8-b9f5-6fe4efe79d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5Eezvzm_hrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a7789a-438a-41d7-a104-c4b84a884ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)\n",
        "#torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha9HPro0eQWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c9b6fa-6e59-4d79-ca9f-10a2f1a0cf21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/1b36eeb1fd7b3a6ec11bf46bde2c38e7e68f71ec774694b9e886c86001aab35d.c483bc3440d25937fdac74506b73b76ee6e67f778a804756214363fc2a1a66ef\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/479aa59074c4dcd4c36106252da033d03bc92e3010947ce1d3714de224c2af1f.7362c0dbb32f750eeea5a5b93bbd0c6876eac41453369265d5a49df1c9142b6f\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06f48b6b3173390d047e15d691fda67ae4ea7733a5eea4b6e0115f5099c4e700.b5cdfa39c63384f94159c36bc9042660c747cea5cf520b43d543bd2c68b3164d\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06f48b6b3173390d047e15d691fda67ae4ea7733a5eea4b6e0115f5099c4e700.b5cdfa39c63384f94159c36bc9042660c747cea5cf520b43d543bd2c68b3164d\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/df2b64a4c86a349ba84354d85b7117b106f2b87085c9bb54cde70d3751907c45.4e3da19dd8adaa6d6a9804bfd45d2dcf17ba544de445847443ef1816bfa3d693\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# –í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π GPT –æ—Ç –°–±–µ—Ä–∞.\n",
        "# Large –∏ medium –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é, –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π colab –Ω–µ —Ç—è–Ω—É–ª,\n",
        "# –ø–æ—ç—Ç–æ–º—É –ø—Ä–∏–¥—ë—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤–µ—Ä—Å–∏–µ–π small (((. \n",
        "\n",
        "model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "\n",
        "#model_name_or_path = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
        "#model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soPxrXgQpFZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62dc7e8-7666-45d4-93f9-aad8a6f7e41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "Loading features from cached file drive/MyDrive/Colab Notebooks/ods/cached_lm_GPT2Tokenizer_64_majakovskiy.txt [took 0.001 s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "\n",
        "# –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ —è –¥–æ–æ–±—É—á–∞–ª –º–æ–¥–µ–ª—å –Ω–∞ —Å—Ç–∏—Ö–∞—Ö –ü—É—à–∫–∏–Ω–∞.\n",
        "# –ù–æ –ø–æ—Ç–æ–º —Ä–µ—à–∏–ª –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –ú–∞—è–∫–æ–≤—Å–∫–æ–≥–æ, —Ç–∞–∫ –∫–∞–∫ —É –Ω–µ–≥–æ –±–æ–ª–µ–µ —É–∑–Ω–∞–≤–∞–µ–º—ã–π —Å—Ç–∏–ª—å.\n",
        "\n",
        "train_path = 'drive/MyDrive/Colab Notebooks/ods/majakovskiy.txt'\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "train_dataset = TextDataset(tokenizer=tokenizer,file_path=train_path,block_size=64)\n",
        "#block_size: It refers to the windows size that is moved across the text file. Set to -1 to use maximum allowed length.\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞–ª–æ–¥–µ—Ä–∞ (–Ω–∞—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–æ –¥–ª–∏–Ω–µ –∫—É—Å–∫–∏)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQH4wBW_pN2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701d3f73-2348-45cc-8d86-4783647e2195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \n",
        "    output_dir=\"drive/MyDrive/Colab Notebooks/ods/output\",\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    \n",
        "    #logging_strategy=\"epoch\",\n",
        "    num_train_epochs=150, # number of training epochs\n",
        "    per_device_train_batch_size=32, # batch size for training\n",
        "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
        "    warmup_steps=10,# number of warmup steps for learning rate scheduler Number of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio.\n",
        "    gradient_accumulation_steps=16, # to make \"virtual\" batch size larger Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5),None) # Optimizer and lr scheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVtvq4sLq9EG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "f1ddf2a4-a6b7-4cf0-f60c-f4e9297fc44f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 24\n",
            "  Num Epochs = 150\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 150\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 01:01, Epoch 150/150]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=150, training_loss=0.09098412831624349, metrics={'train_runtime': 61.8709, 'train_samples_per_second': 58.186, 'train_steps_per_second': 2.424, 'total_flos': 117581414400000.0, 'train_loss': 0.09098412831624349, 'epoch': 150.0})"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "#torch.cuda.empty_cache()\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#print(trainer.state.log_history)\n",
        "#print(type(trainer.state.log_history))\n",
        "#print(len(trainer.state.log_history))\n",
        "#loss_list = []\n",
        "#for element in trainer.state.log_history[:100]:\n",
        "\n",
        "#  loss_list.append(element['loss'])\n",
        "\n",
        "#x = [i for i in range(0,100)]\n",
        "#y = loss_list\n",
        "#plt.plot(x, y)\n",
        "#plt.show() "
      ],
      "metadata": {
        "id": "KJmo_EncvImG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1Vixh9j83ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d79b9a-dbf2-4198-ca2e-a649227f3de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150/config.json\n",
            "Model weights saved in drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
        "model.save_pretrained(\"drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZfBN1meC79e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0866b04d-0292-4285-c091-7dc271e1c0d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading weights file drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "model2 = GPT2LMHeadModel.from_pretrained(\"drive/MyDrive/Colab Notebooks/ods/model/Mayakovsky_150\").to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**–¢–µ–ø–µ—Ä—å –ø–æ–ø—Ä–æ–±—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –ø–æ–ª—É—á–∏–ª–æ—Å—å –ª–∏ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∏–ª—å –ú–∞—è–∫–æ–≤—Å–∫–æ–≥–æ —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏, –æ—Ç—Ä—ã–≤–∏—Å—Ç—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏.**\n",
        "–¢–∞–∫ –∫–∞–∫ –º–µ—Ç—Ä–∏–∫–∏ bertscore –∏ BLEURT —Ç—Ä–µ–±—É—é—Ç –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–≤–∞ —Ç–µ–∫—Å—Ç–∞, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ —Ç–æ—Ç —Å –∫–æ—Ç–æ—Ä—ã–º –±—É–¥–µ–º —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å, –∞ —É –º–µ–Ω—è –Ω–µ –∑–∞–¥–∞—á–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏–ª–∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞, –∞ —á—Ç–æ –Ω–∞–∑—ã–∞–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –Ω—É–ª—è, —Ç–æ –ø—Ä–∏–¥—ë—Ç—Å—è –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"–Ω–∞ –≥–ª–∞–∑\"."
      ],
      "metadata": {
        "id": "rbjCjXHmqS9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=80"
      ],
      "metadata": {
        "id": "j5eYYdPEme-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1)  Greedy Search –æ–Ω –∂–µ –∞—Ä–≥–º–∞–∫—Å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è. –í—ã–±–∏—Ä–∞–µ—Ç—Å—è —Ç–æ–∫–µ–Ω, —É –∫–æ—Ç–æ—Ä–æ–≥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.\n",
        "# –ù–µ –ª—É—á—à–∏–π –º–µ—Ç–æ–¥, —Ç–∞–∫ –∫–∞–∫ —á–∞—Å—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—é —Å–ª–æ–≤. \n",
        "\n",
        "text = \"–í –ª–µ—Å—É —Ä–æ–¥–∏–ª–∞—Å—å –µ–ª–æ—á–∫–∞\\n\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "model2.eval()\n",
        "with torch.no_grad():\n",
        "    out = model2.generate(input_ids, do_sample=False, max_length=max_length)\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "w5eJW9-uqF-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8b29d2-f2ba-4ab2-c4ce-dee9455241ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–í –ª–µ—Å—É —Ä–æ–¥–∏–ª–∞—Å—å –µ–ª–æ—á–∫–∞\n",
            "–∏ –≤ –¥–æ—Ä–æ–≥—É\n",
            "—É–±–µ–∂–∞–ª–∞ –µ–ª–æ—á–∫–∞.\n",
            "–ö –Ω–∞–º –≤ –≥–æ—Å—Ç–∏\n",
            "–ø—Ä–∏—à–µ–ª –ù–æ–≤—ã–π –≥–æ–¥.\n",
            "–û–Ω –±–æ–ª—å—à–æ–π, –∫—Ä–∞—Å–∏–≤—ã–π,\n",
            "–∏ –¥–æ–±—Ä—ã–π.\n",
            "–°–Ω–µ–≥–æ–≤–∏–∫\n",
            "—É–±–µ–∂–∞–ª, —Ö–ª–æ–ø–Ω—É–≤ –¥–≤–µ—Ä—å—é.\n",
            "–ê —É –Ω–∞—Å\n",
            "–≤ –¥–æ–º–µ\n",
            "–≤–∑–æ—Ä–≤–∞–ª—Å—è –±–∏–¥–æ–Ω\n",
            "—Å —Å–æ–ª–µ–Ω—ã—Ö –æ–≥—É—Ä—Ü–æ–≤.\n",
            "–í—Å–µ –≤ –¥–æ–º–µ\n",
            "–ø–ª–µ—Å–Ω–µ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2)  Beam search –æ–Ω –∂–µ –ø–æ–∏—Å–∫ –ø–æ –ª—É—á—É. –ó–¥–µ—Å—å –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ\n",
        "# —Ç–æ–∫–µ–Ω–æ–≤ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç —à–∏—Ä–∏–Ω—ã –ª—É—á–∞) –∏ –¥–∞–ª—å—à–µ –ø–æ–∏—Å–∫ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤.\n",
        "# –í –∏—Ç–æ–≥–µ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –≤–∞—Ä–∏–∞–Ω—Ç —Å –ª—É—á—à–µ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–µ–π.\n",
        "\n",
        "text = \"–í –ª–µ—Å—É —Ä–æ–¥–∏–ª–∞—Å—å –µ–ª–æ—á–∫–∞\\n\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model2.generate(input_ids, do_sample=True, num_beams=4, max_length=max_length)\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "DQRYPIaYrU5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7cb8d9f-194f-4c38-ae51-c9d066b5d86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–í –ª–µ—Å—É —Ä–æ–¥–∏–ª–∞—Å—å –µ–ª–æ—á–∫–∞\n",
            "–∏ –≤ –¥–æ—Ä–æ–≥—É\n",
            "—É–±–µ–∂–∞–ª–∞ –µ–ª–æ—á–∫–∞.\n",
            "–ö –Ω–∞–º –≤ –≥–æ—Å—Ç–∏\n",
            "–ø—Ä–∏—à–µ–ª –î–µ–¥ –ú–æ—Ä–æ–∑\n",
            "–∏ —Å–∫–∞–∑–∞–ª:\n",
            "‚Äî –î–µ—Ç–∏—à–∫–∏,\n",
            "–∫–∞–∂–¥—ã–π –¥–µ–Ω—å\n",
            "–Ω–∞–¥ –∫—Ä—ã—à–∞–º–∏\n",
            "–≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤\n",
            "–ø—Ä–æ–Ω–æ—Å—è—Ç—Å—è —Ä–æ–π\n",
            "—á–µ—Ä–≤–µ–π –∏ –∫–æ–º–∞—Ä–æ–≤.\n",
            "–ö–∞–∂–¥—É—é –Ω–æ—á—å\n",
            "–Ω–∞–¥ –∫—Ä—ã—à–∞–º–∏\n",
            "–≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤\n",
            "–ø—Ä–æ–Ω–æ—Å—è—Ç—Å—è\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–∏–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç—É \"—á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏\"\n",
        "# –∏ –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –±–µ—Ä–µ—Ç—Å—è –Ω–µ —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω, –∞ –≤—ã–±–æ—Ä \n",
        "# –¥–µ–ª–∞–µ—Ç—Å—è \"—Å–ª—É—á–∞–π–Ω–æ\" —Å —É—á–µ—Ç–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏. –ü–∞—Ä–∞–º–µ—Ç—Ä —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã\n",
        "# –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–µ–ø–µ–Ω—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏. –ü—Ä–∏ –Ω—É–ª–µ–≤–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –∞—Ä–≥–º–∞–∫—Å–Ω–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ.\n",
        "# –ê –ø—Ä–∏ –±–æ–ª—å—à–æ–π –Ω–∞—É–≥–∞–¥. –°–æ–≤–µ—Ç—É—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –æ—Ç 0.8 –¥–æ 2.\n",
        "\n",
        "text = \"–í –ª–µ—Å—É —Ä–æ–¥–∏–ª–∞—Å—å –µ–ª–æ—á–∫–∞\\n\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model2.generate(input_ids, do_sample=True,temperature=1.1, max_length=max_length)\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "AiFIfsUAsPOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36216dd5-d658-4e4b-9ed8-a98d21353ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–í –ª–µ—Å—É —Ä–æ–¥–∏–ª–∞—Å—å –µ–ª–æ—á–∫–∞\n",
            "–∏ –≤–æ –≤—Å–µ –¥–≤–µ—Ä–∏\n",
            "–≤–µ–∂–ª–∏–≤—ã–º–∏\n",
            "–Ω–æ—Ç–∞–º–∏\n",
            "—Å–æ–æ–±—â–∞—é—Ç:\n",
            "—ç—Ç–æ ‚Äî\n",
            "–ø–ª–æ—Ö–æ\n",
            "–∏ –≤–ø–æ–ª–Ω–µ\n",
            "–¥–∞–∂–µ\n",
            "–ø–ª–æ—Ö–æ\n",
            "–¥–ª—è –¥–µ—Ç–µ–π‚Ä¶\n",
            "–í—Å–µ, —á—Ç–æ –Ω—É–∂–Ω–æ\n",
            "–¥–æ–º–∞—à–Ω–µ–º—É —Ä–µ–±–µ–Ω–∫—É,\n",
            "–¥–æ—Å—Ç–∞–µ—Ç\n",
            "–º–∞–º–∞\n",
            "–∏–∑ –ø–æ–≥—Ä–µ–±–∞,\n",
            "–∞ –º–∞–ª–µ–Ω—å–∫–∏–π —Ä–µ–±–µ–Ω–æ–∫\n",
            "–¥–æ—Å—Ç–∞–µ—Ç\n",
            "–¥—Ä–æ–∂–∞–ª–∫—É\n",
            "–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç\n",
            "–ø–æ–∏—Ç—å\n",
            "–º–æ—é\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Nucleus Sampling. –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.\n",
        "# –î–ª—è –∑–∞–ø—Ä–µ—Ç–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤–≤–æ–¥—è—Ç top-k –∏–ª–∏ top-p –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è.\n",
        "# –ü—Ä–∏ top-k –∑–∞–Ω—É–ª—è—é—Ç—Å—è –≤—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫—Ä–æ–º–µ k —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö.\n",
        "# –ü—Ä–∏ top-p –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–∞–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤, —Å—É–º–º–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±–æ–ª—å—à–µ p. \n",
        "\n",
        "text = \"–í –ª–µ—Å—É —Ä–æ–¥–∏–ª–∞—Å—å –µ–ª–æ—á–∫–∞\\n\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model2.generate(input_ids, do_sample=True,temperature=1.2, top_k=25,\n",
        "                     top_p=0.8, max_length=max_length)\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "wbNyRgiqtoXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceff14ce-4d30-4c31-8524-e09baef91cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–í –ª–µ—Å—É —Ä–æ–¥–∏–ª–∞—Å—å –µ–ª–æ—á–∫–∞\n",
            "–∏ –≤ –æ–∫–æ—à–∫–æ\n",
            "–∑–≤–µ–∑–¥–æ—é\n",
            "–≤—ã–ª–µ—Ç–µ–ª –∑–∞–π—á–∏–∫.\n",
            "–ò –≤—Å–µ –∑–∞—Å–º–µ—è–ª–∏—Å—å\n",
            "–Ω–∞–¥ –∑–∞–π—á–∏–∫–æ–º\n",
            "–∏ –Ω–∞–∑–≤–∞–ª–∏ –µ–≥–æ\n",
            "–ó–∞–π—á–∏–∫–æ–º.\n",
            "\n",
            "–ó–∞–π—á–∏–∫\n",
            "–≤–µ—Å–µ–ª–∏—Ç—Å—è,\n",
            "—É–ª—ã–±–∞—è—Å—å,\n",
            "–∏ –≥–æ–≤–æ—Ä–∏—Ç:\n",
            "¬´–ó–∞–π–∫–∞,\n",
            "—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π,\n",
            "–ø–æ–π–¥—É\n",
            "–∫ —Ç–µ–±–µ\n",
            "–∏ —Å—Ä–∞–∑—É —Å–ø—Ä—è—á—É—Å—å.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–°–º—ã—Å–ª –ø–æ–ª—É—á–∏–≤—à–∏—Ö—Å—è —Ç–µ–∫—Å—Ç–æ–≤ –∫–æ–Ω–µ—á–Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ—Ç –∂–µ–ª–∞—Ç—å –ª—É—á—à–µ–≥–æ, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —à–∏–∑–æ—Ñ—Ä–µ–Ω–∏—é. –ù–æ —Å–µ—Ç–∏ —É–¥–∞–ª–æ—Å—å \"—É—Ö–≤–∞—Ç–∏—Ç—å\" —Å—Ç–∏–ª—å –ú–∞—è–∫–æ–≤—Å–∫–æ–≥–æ —Å \n",
        "–∫–æ—Ä–æ—Ç–∫–∏–º–∏, –æ—Ç—Ä—ã–≤–∏—Å—Ç—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏. –í–æ–∑–º–æ–∂–Ω–æ —Å –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω–æ–π –º–æ–¥–µ–ª—å—é –≤ —Ç–µ–∫—Å—Ç–µ –±—ã–ª–æ –±—ã –±–æ–ª—å—à–µ —Å–º—ã—Å–ª–∞. "
      ],
      "metadata": {
        "id": "CAL7Kv8zGJE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–°—Ç–∞—Ç—å—è –ø—Ä–æ BLEURT\n",
        "https://neurohive.io/ru/novosti/bleurt-metrika-dlya-ocenki-modelej-dlya-generacii-teksta/ \n",
        "\n",
        "–ú–µ—Ç—Ä–∏–∫–∏, —Å—ç–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ.\n",
        "https://spb.hse.ru/mirror/pubs/share/480745430.pdf   \n",
        "\n",
        "–ë–µ—Ä—Ç—Å–∫–æ—Ä\n",
        "https://colab.research.google.com/drive/1kpL8Y_AnUUiCxFjhxSrxCsc6-sDMNb_Q  \n",
        "\n",
        "–ë–µ—Ä—Ç—Å–∫–æ—Ä\n",
        "https://paperswithcode.com/paper/bertscore-evaluating-text-generation-with \n",
        "\n",
        "–°—Ç–∞—Ç—å—è –ø—Ä–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞\n",
        "https://huggingface.co/blog/how-to-generate\n",
        "\n",
        "–ë–µ—Ä—Ç—Å–∫–æ—Ä\n",
        "https://github.com/huggingface/datasets/blob/master/metrics/bertscore/bertscore.py\n",
        "\n",
        "Fine-tune a non-English GPT-2 Model with Huggingface\n",
        "https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb"
      ],
      "metadata": {
        "id": "YhkTF24by-yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zssXL0SizPgo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "project_colab_Mayakovsky.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}